{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\renewcommand{\\vvec}[2]{\\left[ \\begin{array}{c} \\mathbf{#1}\\\\ \\mathbf{#2} \\end{array}\\right]}\n",
    "\\renewcommand{\\mmat}[4]{\\left[ \\begin{array}{cc} \\mathbf{#1}&\\mathbf{#2}\\\\ \\mathbf{#3}&\\mathbf{#4} \\end{array}\\right]}\n",
    "\\renewcommand{\\aaa}{\\mathbf{a}}\n",
    "\\renewcommand{\\AAA}{\\mathbf{A}}\n",
    "\\renewcommand{\\xyvec}{\\left[ \\begin{array}{c} \\xx\\\\\\yy \\end{array} \\right]}\n",
    "\\renewcommand{\\xyvecc}{\\left[ \\begin{array}{c} x^1\\\\y^1 \\end{array} \\right]}\n",
    "\\renewcommand{\\mm}{\\mathbf{m}}\n",
    "\\renewcommand{\\xx}{\\mathbf{x}}\n",
    "\\renewcommand{\\yy}{\\mathbf{y}}\n",
    "\\renewcommand{\\zz}{\\mathbf{z}}\n",
    "\\renewcommand{\\vv}{\\mathbf{v}}\n",
    "\\renewcommand{\\ee}{\\mathbf{e}}\n",
    "\\renewcommand{\\ww}{\\mathbf{w}}\n",
    "\\renewcommand{\\XX}{\\mathbf{X}}\n",
    "\\renewcommand{\\YY}{\\mathbf{Y}}\n",
    "\\renewcommand{\\WW}{\\mathbf{W}}\n",
    "\\renewcommand{\\VV}{\\mathbf{V}}\n",
    "\\renewcommand{\\DD}{\\mathbf{D}}\n",
    "\\renewcommand{\\dd}{\\mathbf{d}}\n",
    "\\renewcommand{\\ZZ}{\\mathbf{Z}}\n",
    "\\renewcommand{\\CC}{\\mathbf{C}}\n",
    "\\renewcommand{\\bbeta}{\\boldsymbol{\\mathbf{\\beta}}}\n",
    "\\renewcommand{\\ddelta}{\\boldsymbol{\\mathbf{\\delta}}}\n",
    "\\renewcommand{\\mmu}{\\boldsymbol{\\mathbf{\\mu}}}\n",
    "\\renewcommand{\\ssigma}{\\boldsymbol{\\mathbf{\\sigma}}}\n",
    "\\renewcommand{\\reals}{\\mathbb{R}}\n",
    "\\renewcommand{\\loglik}{\\mathcal{LL}}\n",
    "\\renewcommand{\\penloglik}{\\mathcal{PLL}}\n",
    "\\renewcommand{\\likelihood}{\\mathcal{L}}\n",
    "\\renewcommand{\\Data}{\\textrm{Data}}\n",
    "\\renewcommand{\\given}{ \\big| }\n",
    "\\renewcommand{\\MLE}{\\textrm{MLE}}\n",
    "\\renewcommand{\\EE}{\\mathbb{E}}\n",
    "\\renewcommand{\\EEE}{\\mathbf{E}}\n",
    "\\renewcommand{\\KL}{\\textrm{KL}}\n",
    "\\renewcommand{\\Bound}{\\mathcal{B}}\n",
    "\\renewcommand{\\tth}{\\textrm{th}}\n",
    "\\renewcommand{\\Gaussian}[2]{\\mathcal{N}\\left(#1,#2\\right)}\n",
    "\\renewcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\renewcommand{\\ones}{\\mathbf{1}}\n",
    "\\renewcommand{\\corr}[2]{\\textrm{corr}(#1,#2)}\n",
    "\\renewcommand{\\diag}[1]{\\textrm{diag}\\left( #1 \\right)}\n",
    "\\renewcommand{\\sigmoid}[1]{\\sigma\\left(#1\\right)}\n",
    "\\renewcommand{\\myexp}[1]{\\exp\\left\\{#1\\right\\}}\n",
    "\\renewcommand{\\mylog}[1]{\\log\\left\\{#1\\right\\}}\n",
    "\\renewcommand{\\argmax}{\\mathop{\\textrm{argmax}}}\n",
    "\\renewcommand{\\new}{\\textrm{new}}\n",
    "\\renewcommand{\\old}{\\textrm{old}}\n",
    "\\renewcommand{\\bb}{\\mathbf{b}}\n",
    "\\renewcommand{\\ba}{\\mathbf{a}}\n",
    "\\renewcommand{\\bg}{\\mathbf{g}}\n",
    "\\renewcommand{\\BB}{\\mathbf{B}}\n",
    "\\renewcommand{\\BA}{\\mathbf{A}}\n",
    "\\renewcommand{\\BC}{\\mathbf{C}}\n",
    "\\renewcommand{\\UU}{\\mathbf{U}}\n",
    "\\renewcommand{\\uu}{\\mathbf{u}}\n",
    "\\renewcommand{\\hh}{\\mathbf{h}}\n",
    "\\renewcommand{\\SSS}{\\mathbf{S}}\n",
    "\\renewcommand{\\sss}{\\mathbf{s}}\n",
    "\\renewcommand{\\rr}{\\mathbf{r}}\n",
    "\\renewcommand{\\tr}[1]{\\textrm{tr}\\left\\{#1\\right\\}}\n",
    "\\renewcommand{\\argmin}{\\mathop{\\textrm{argmin}}}\n",
    "\\renewcommand{\\abs}[1]{\\left\\lvert#1\\right\\rvert}\n",
    "\\renewcommand{\\sign}[1]{\\textrm{sign}\\left(#1\\right)}\n",
    "\\renewcommand{\\minimize}{\\mathop{\\textrm{minimize}}}\n",
    "\\renewcommand{\\subjectto}{\\mathop{\\textrm{subject to}}}\n",
    "\\renewcommand{\\relu}{\\textrm{ReLU}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Background\n",
    "The Federalist Papers is a collection of 85 articles written under pseudonym Publius. The authors of these articles were Alexander Hamilton, James Madison and John Jay. A number of these articles are not clearly attributable to an author. Further, some articles were products of collaboration between authors. \n",
    "\n",
    "## Task\n",
    "We will try to attribute articles to authors. We are going to cluster the articles. To accomplish this, we will use two models.\n",
    "\n",
    "1. We will model the word counts using mixture of product of Poissons. For each cluster and word combination we will have a single poisson distribution. \n",
    "2. We will model transformed word counts using mixture of multivariate Gaussians with diagonal covariance.\n",
    "\n",
    "We will use articles with known authors to assess our clusters. Specifically, we will perform a statistical test for each cluster and each author that will tell us how likely is it that articles by the same author end up in the cluster by chance. The null hypothesis -- that we are trying to reject -- is that the assignment of articles to clusters is completely random and there is no association between our clusters and authors of articles. We compute probability that the assignment of authors to clusters is generated randomly. If this probability is low, then we can reject the null hypothesis and claim that association between authors and clusters is significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# load data\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "    kwargs = {}\n",
    "except:\n",
    "    import _pickle as pickle\n",
    "    kwargs = {'encoding':'bytes'}\n",
    "import gzip\n",
    "import scipy\n",
    "\n",
    "with gzip.open(\"preprocessed_documents.pgz\") as f:\n",
    "    documents, counter = pd.read_pickle(f, compression=None)\n",
    "\n",
    "dataMat = scipy.sparse.vstack([doc['acounts'] for doc in documents])\n",
    "dataMat = np.asarray(dataMat.todense().astype('int32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use counts of functional words as features for clustering the articles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of used features: 199, they are ['about' 'above' 'accordingly' 'after' 'against' 'all' 'along' 'also'\n",
      " 'although' 'amidst' 'among' 'amongst' 'an' 'and' 'another' 'anti' 'any'\n",
      " 'anything' 'are' 'around' 'as' 'aside' 'at' 'bar' 'be' 'because' 'been'\n",
      " 'before' 'behind' 'below' 'beneath' 'besides' 'between' 'beyond' 'both'\n",
      " 'but' 'by' 'can' 'certain' 'concerning' 'consequently' 'considering'\n",
      " 'could' 'dare' 'do' 'down' 'during' 'each' 'either' 'enough' 'even'\n",
      " 'every' 'everything' 'except' 'excluding' 'failing' 'few' 'fewer'\n",
      " 'following' 'for' 'from' 'given' 'had' 'has' 'have' 'he' 'hence' 'her'\n",
      " 'hers' 'herself' 'him' 'himself' 'his' 'however' 'if' 'in' 'including'\n",
      " 'inside' 'into' 'is' 'it' 'its' 'itself' 'less' 'like' 'little' 'many'\n",
      " 'may' 'me' 'might' 'mine' 'more' 'most' 'much' 'must' 'my' 'myself'\n",
      " 'near' 'neither' 'nevertheless' 'no' 'none' 'nor' 'not' 'nothing'\n",
      " 'notwithstanding' 'now' 'of' 'off' 'on' 'once' 'one' 'only' 'opposite'\n",
      " 'or' 'other' 'ought' 'our' 'ours' 'ourselves' 'out' 'over' 'part' 'past'\n",
      " 'per' 'regarding' 'respecting' 'round' 'save' 'saving' 'several' 'shall'\n",
      " 'she' 'should' 'since' 'so' 'some' 'something' 'such' 'than' 'that' 'the'\n",
      " 'their' 'theirs' 'them' 'themselves' 'then' 'thence' 'there' 'therefore'\n",
      " 'these' 'they' 'things' 'this' 'those' 'though' 'through' 'throughout'\n",
      " 'thus' 'till' 'to' 'toward' 'towards' 'under' 'unless' 'unlike' 'until'\n",
      " 'up' 'upon' 'us' 'various' 'wanting' 'was' 'we' 'were' 'what' 'whatever'\n",
      " 'when' 'whenever' 'where' 'whereas' 'wherever' 'whether' 'which' 'while'\n",
      " 'whilst' 'who' 'whoever' 'whom' 'whose' 'will' 'with' 'within' 'without'\n",
      " 'would' 'yet' 'you' 'your' 'yourselves']\n"
     ]
    }
   ],
   "source": [
    "# load functional the word list\n",
    "import codecs\n",
    "with codecs.open('updated_functionWords.txt') as f:\n",
    "    function_words = f.read().splitlines() \n",
    "# find interaction between the list and the words in the documents\n",
    "func_lst = np.nonzero(np.in1d(counter.get_feature_names(),(function_words)))[0]\n",
    "lst = func_lst\n",
    "words = np.array(counter.get_feature_names())[lst]\n",
    "print('# of used features: {}, they are {}'.format(len(lst), words))\n",
    "dataMat_selected = np.asarray(dataMat[:,lst])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Mixture of Poissons \n",
    "We will assume that each article belongs to a single cluster.\n",
    "Since we are modeling word counts, a natural modeling choice is to assume that counts are distributed according to the Poisson distribution. Each cluster will have a Poisson distribution associated with each word.\n",
    "\n",
    "We will derive the formulation of mixture of Poissons model and fit it to the data.\n",
    "\n",
    "1) Possion pmf $$p(k \\mid \\lambda) = \\frac{\\lambda^{k} e^{-\\lambda}}{k!}$$ tells us probability of observing a count of $k$ for specific $\\lambda$ value\n",
    "\n",
    "2) Notation for the model\n",
    "  1. $x_i$ be the feature vector of the $i^{th}$ sample, $x_{i, j}$ be the $j^{th}$ features of the $i^{th}$ sample. \n",
    "  2. $h_i$ be the index of the cluster for the $i^{th}$ sample. \n",
    "  3. $\\lambda_m$ be the lambda vector for the $m^{th}$ cluster, $\\lambda_{m,j}$ be the parameters of Possion pmf for cluster c and feature j.\n",
    "  4. $p(h_i = c) = \\pi_c$\n",
    "  5. $p(x_i \\mid h_i = m, \\lambda) = \\prod_j p(x_{i,j}\\mid \\lambda_{m,j})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) **[1/2pt]** Write out the log probability of sample $x_i$ given that it belongs to cluster $m$, ($h_i = m$):\n",
    "\n",
    "$$\n",
    "\\log p(x_i| h_i = m, \\lambda) = \\sum_j  \\log p(x_{i,j}|\\lambda_{m,j}) = \\sum_j x_{i,j} \\log (\\lambda_{m,j}) - \\lambda_{m,j} - \\log (x_{i,j}!)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) **[1/2pt]** Write out the log of marginal probability of sample $x_i$ in terms of $p(x_i\\mid\\lambda,h_i)$ and $\\pi$\n",
    "\n",
    "$$\n",
    "\\log p(x_i \\mid \\lambda,\\pi) = \\log \\sum_c \\pi_c p(x_i | h_i = c, \\lambda)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) **[1pt]** Write out the log-likelihood using the above log-probability for all samples.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "LL(\\lambda,\\pi) &= \\log\\Big( \\prod_{i} p(x_{i})\\Big)\\\\\n",
    "&= \\sum_{i}  \\log p(x_{i,j}|\\lambda_{m,j})\\\\\n",
    "& = \\sum_{i} \\log p(x_i|\\lambda,\\pi)\\\\\n",
    "& = \\sum_{i} \\log \\sum_{c} \\pi_{c} p(x_{i}|h_{i} = c,\\pi,\\lambda)\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) **[1pt]** Apply Jensen’s inequality to derive a lower-bound.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "LL(\\lambda, \\pi) = \\log( \\prod_{i} p(x_{i})) &= \\sum_{i} \\log \\left \\{ \\sum_{c} q(h_i=c) \\frac{ p(x_{i}, h_i = c) }{q(h_i=c)} \\right \\} \\\\\n",
    "& \\ge \\sum_{i} \\sum_c q(h_i = c) \\log \\left\\{ \\frac{p(x_i, h_i = c)}{q(h_i = c)} \\right\\} \\\\\n",
    "&= \\sum_{i} \\sum_c q(h_i = c) \\log \\left\\{  p(x_i, h_i = c) \\right\\} - \\sum_{i} \\sum_c q(h_i = c)  \\log \\left\\{ q(h_i = c) \\right\\}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "To check your answer, plug-in $p(h_i=c \\mid x_i)$ in place of $q(h_i=c)$. You should recover the log-likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) **[1pt]** If we let $q(h_i)$ be the posterior probability $p(h_i\\mid x_i,\\lambda, \\pi)$, we maximize the lower-bound. Use Bayes rule to derive posterior.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(h_i = m\\mid x_i, \\lambda, \\pi) &= \\frac{p(h_i=m,x_i|\\lambda,\\pi)}{p(x_i|\\lambda,\\pi)} \\\\\\\\\n",
    "&= \\frac{p(h_i=m|\\pi)p(x_i|\\lambda,h_i=m)}{\\sum_{c}p(h_i=c|\\pi)p(x_i|\\lambda,h_i=c)}\\\\\\\\\n",
    "&= \\frac{\\pi_{m}p(x_i|\\lambda,h_i=m)}{\\sum_{c}\\pi_{c}p(x_i|\\lambda,h_i=c)}\\\\\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) **[1pt]** Implement a function that computes the log probability of a single sample. Note that for computing log factorial you need to use the function we provide.\n",
    "Due to numerical precision, we compute probability in log domain. First implement the function to compute $\\log p(x_i \\mid \\lambda_m)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsum(lp):\n",
    "    m = np.max(lp)\n",
    "    return np.log(np.sum(np.exp(lp-m))) + m\n",
    "\n",
    "# you cannot compute the factorial directly for large x, compute it in log domain\n",
    "def logfactorial(x):\n",
    "    return np.sum(np.log(np.arange(1,x+1)))\n",
    "\n",
    "# xs: a vector for x_i\n",
    "# ls: a vector for lambdas\n",
    "# return log probability of equation \n",
    "def logprobvec(xs, ls, compute_factorial = False): \n",
    "    logfactorial_val = np.zeros((len(xs,)))\n",
    "    if compute_factorial:\n",
    "        for i in np.arange(len(xs)):\n",
    "            logfactorial_val[i] = logfactorial(xs[i])\n",
    "    lp = xs*np.log(ls) - ls - logfactorial_val\n",
    "    lp = np.sum(lp)\n",
    "    return lp\n",
    "\n",
    "# test function\n",
    "test_x = np.array([5, 33, 211, 474])\n",
    "test_l = np.array([4, 60, 300, 600])\n",
    "res_v = logprobvec(test_x, test_l, False)\n",
    "assert(np.allclose(res_v, 3413.687601))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) **[1pt]** Implement a function that computes the log posterior for all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xs: an array of shape N*F (N, # samples, F, # features)\n",
    "# lambdas: an array of shape K*F (K, # clusters, F, # features)\n",
    "# pis: a vector of shape K\n",
    "# return three variables:\n",
    "# logprobs: an array of shape K*N, the log posterior probabilities in (7), the log probability\n",
    "#           of a sample belong to each cluster\n",
    "# loglik: log-likelihood of the model in 4)\n",
    "# labels: a vector of shape N, the most probable cluster each sample belongs to\n",
    "def logposterior_MP(xs, lambdas, pis):\n",
    "    K = lambdas.shape[0]\n",
    "    F = lambdas.shape[1]\n",
    "    N = xs.shape[0]\n",
    "    assert(xs.shape[1] == F)\n",
    "    logprobs = np.zeros((K,N))\n",
    "    loglik = 0\n",
    "    labels = np.zeros(N)\n",
    "    for n in range(N):\n",
    "        x = xs[n,:]\n",
    "        for k in range(K):\n",
    "            ls = lambdas[k,:]\n",
    "            logprobs[k,n] = np.log(pis[k])+logprobvec(x,ls,False)\n",
    "        docloglik = logsum(logprobs[:,n])\n",
    "        loglik = loglik + docloglik\n",
    "        logprobs[:,n] = logprobs[:,n] - docloglik\n",
    "        labels[n] = np.argmax(logprobs[:,n])\n",
    "    return logprobs, loglik, labels\n",
    "\n",
    "# test function\n",
    "test_x = np.array([[7, 4, 6], [2, 8, 1], [3, 3, 9]], dtype='float32')\n",
    "test_l = np.tile(np.round(np.mean(test_x, axis=0)), (3,1))\n",
    "np.random.seed(10)\n",
    "test_l = test_l + 1 + np.abs(np.random.randn(test_l.shape[0], test_l.shape[1]))*2\n",
    "test_pis = np.array([0.33, 0.33, 0.33])\n",
    "res_lp, res_ll, res_lab = logposterior_MP(test_x, test_l, test_pis)\n",
    "assert(np.allclose(res_lp, [[-1.57270506, -4.35048081, -2.08895623],\n",
    "                            [-1.3579241 , -1.1180958 , -0.75511075],\n",
    "                            [-0.62488554, -0.41521594, -0.90084777]]))\n",
    "assert(np.allclose(res_ll, 21.969895))\n",
    "assert(np.allclose(res_lab, [2,2,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10) **[1pt]** Derive the update formulation for $\\lambda_{m, j}$. Take the derivative of the lower-bound on log-likelihood.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial \\lambda_{m,j}} \\sum_{i} \\sum_{j} \\sum_{c} q(h_i=c) \\log \\left \\{  p(x_{i,j}, h_i = c) \\right \\} &=\n",
    "\\frac{\\partial}{\\partial \\lambda_{m,j}} \\sum_{i} \\sum_{j} \\sum_{c} \\log \\{ p(h_i=c) \\} + \\log \\{ p(x_{i,j}|h_i=c)\\}  \\\\\n",
    "&=\n",
    "\\frac{\\partial}{\\partial \\lambda_{m,j}} \\sum_{i} \\sum_{j} \\sum_{c} q(h_i=c) \\log\\left\\{ \\frac{\\lambda_{m,j}^{x_{i,j}} e^{-\\lambda_{m,j}}}{x_{i,j}!}\\right\\}  \\\\\n",
    "&=\n",
    "\\frac{\\partial}{\\partial \\lambda_{m,j}} \\sum_{i} \\sum_{j} \\sum_{c} q(h_i=c) (x_{i,j}\\log\\{\\lambda_{m,j}\\}-\\lambda_{m,j}-\\log\\{x_{i,j}!\\})  \\\\\n",
    "&=\n",
    "\\sum_i\\sum_j \\Big( \\frac{x_{i,j}}{\\lambda_{m,j}}-1 \\Big)\\sum_c q(h_i=c)  \\\\\n",
    "&= \\sum_{i}  q(h_i=m) \\left( \\frac{x_{i,j}}{\\lambda_{m,j}} - 1 \\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Let the derivative be zero, we have:\n",
    "\n",
    "$$\n",
    "\\lambda_{m,j} = \\frac{ \\sum_i q(h_i=m) x_{i,j} }{\\sum_i q(h_i=m)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11) **[1pt]** Derive the update formulation for $\\pi_m$. Take the derivative of the lower-bound. $\\big($Note: $\\sum_c \\pi_c = 1\\big)$\n",
    "\n",
    "$$\n",
    "\\text{Lagrangian}\\> L(\\pi, \\gamma) = \\sum_{i}  q(h_i=m) \\log \\left\\{ \\pi_m \\right\\} + \\gamma\\Big(\\sum_c \\pi_c - 1\\Big)\n",
    "$$\n",
    "\n",
    "Taking derivatives on both $\\pi_m$ and $\\gamma$ and setting them to zero, we have:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    &\\sum_i \\frac{q(h_i=m)}{\\pi_m} + \\gamma = 0 \\\\\n",
    "    &\\sum_c \\pi_c= 1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "From above, we have:\n",
    "\n",
    "$$\n",
    "\\pi_m = \\frac{ \\sum_i q(h_i=m) }{N}\n",
    "$$\n",
    "\n",
    "where N is the number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12) **[1pt]**  Write the code to update $\\lambda_{m,j}$ and $\\pi_m$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qs: an array of shape K*N, the posterior probabilities in (7), the probability of a sample belong to each cluster\n",
    "# return two variables:\n",
    "# ls: the updated lambda, an array of shape K*F (K, # clusters, F, # features)\n",
    "# pis: the updated pi, an vector of shape K\n",
    "def update_MP(qs, xs):\n",
    "    suff = np.dot(qs,xs)\n",
    "    # we add small constant to avoid division by zero\n",
    "    ls = (suff+0.001)/(0.001+np.sum(qs,1)[:,np.newaxis])\n",
    "    pis = np.sum(qs,axis=1)\n",
    "    pis = pis/np.sum(qs)\n",
    "    return ls, pis\n",
    "\n",
    "# test function\n",
    "test_qs = np.array([[.9, .3],\n",
    "                    [.05, .4],\n",
    "                    [.05, .3]])\n",
    "test_xs = np.array([[7,4,6], [2,8,1]], dtype='float32')\n",
    "res_ls, res_pis = update_MP(test_qs, test_xs)\n",
    "assert(np.allclose(res_ls,[[5.746044, 4.996669, 4.74687 ],\n",
    "                            [2.552106, 7.541019, 1.554323],\n",
    "                            [2.709401, 7.410256, 1.712250]]))\n",
    "assert(np.allclose(res_pis, [0.6, 0.225, 0.175]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "13)  Run the code below, which will invoke your E-step (```logposterior_MP```) and M-step (```update_MP```)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting on data of size N: 85 and L: 199\n",
      "\n",
      " A fit with better log-likelihood (250570.84708075694) found for seed 0\n",
      "cluster 0\n",
      "(2 article(s) by DISPUTED), (18 article(s) by HAMILTON), \n",
      "cluster 1\n",
      "(8 article(s) by DISPUTED), (9 article(s) by HAMILTON), (1 article(s) by JAY), (3 article(s) by MADISON), \n",
      "cluster 2\n",
      "(1 article(s) by DISPUTED), (10 article(s) by HAMILTON), (11 article(s) by MADISON), \n",
      "cluster 3\n",
      "(4 article(s) by DISPUTED), (14 article(s) by HAMILTON), (4 article(s) by JAY), \n",
      "\n",
      " A fit with better log-likelihood (250708.9882702828) found for seed 1\n",
      "cluster 0\n",
      "(4 article(s) by DISPUTED), (11 article(s) by HAMILTON), (4 article(s) by JAY), \n",
      "cluster 1\n",
      "(1 article(s) by DISPUTED), (8 article(s) by HAMILTON), (4 article(s) by MADISON), \n",
      "cluster 2\n",
      "(3 article(s) by DISPUTED), (26 article(s) by HAMILTON), (1 article(s) by MADISON), \n",
      "cluster 3\n",
      "(7 article(s) by DISPUTED), (6 article(s) by HAMILTON), (1 article(s) by JAY), (9 article(s) by MADISON), \n",
      "Best\n",
      "cluster 0\n",
      "(4 article(s) by DISPUTED), (11 article(s) by HAMILTON), (4 article(s) by JAY), \n",
      "cluster 1\n",
      "(1 article(s) by DISPUTED), (8 article(s) by HAMILTON), (4 article(s) by MADISON), \n",
      "cluster 2\n",
      "(3 article(s) by DISPUTED), (26 article(s) by HAMILTON), (1 article(s) by MADISON), \n",
      "cluster 3\n",
      "(7 article(s) by DISPUTED), (6 article(s) by HAMILTON), (1 article(s) by JAY), (9 article(s) by MADISON), \n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "def report_labels(labels, side_info):\n",
    "    for l in np.unique(labels):\n",
    "        print('cluster {0:d}'.format(int(l)))\n",
    "        members = np.nonzero(labels==l)\n",
    "        tmp = np.sort(np.array(side_info)[members])\n",
    "        tmp = Counter(tmp)\n",
    "        for z in tmp.keys():\n",
    "            author = z\n",
    "            if z == '':\n",
    "                author = 'DISPUTED'\n",
    "            print('({} article(s) by {}),'.format(tmp[z], author), end=' ')\n",
    "        print()\n",
    "\n",
    "def fit(xs, K, side_info):\n",
    "    L = xs.shape[1]\n",
    "    N = xs.shape[0]\n",
    "    print(\"Fitting on data of size N: {} and L: {}\".format(N, L))\n",
    "    best_loglik = -1e+308\n",
    "    best_logliks = []\n",
    "    best_labels = []\n",
    "    best_ls = []\n",
    "    best_pis = []\n",
    "    verbose = False\n",
    "    for s in range(100): # run the algorithm 100 times\n",
    "        np.random.seed(s)\n",
    "        ls = np.mean(xs,0)*(1.0 + 0.5*(np.random.rand(K,L)-0.5)) # each time initialize with different lambda\n",
    "        pis = [1./K]*K\n",
    "        logliks = []\n",
    "        for it in range(50): # each time run for 50 iterations\n",
    "            logqs, loglik, labels = logposterior_MP(xs, ls, pis)\n",
    "            qs = np.exp(logqs)\n",
    "            logliks.append(loglik)\n",
    "            ls,pis = update_MP(qs,xs)\n",
    "        if verbose:\n",
    "            print('\\n Seed {}: log-likelihood = {:.5}, best log-likelihood so far = {:.5}'.format(s+1, loglik, best_loglik))\n",
    "        if loglik > best_loglik:\n",
    "            best_loglik = loglik\n",
    "            print('\\n A fit with better log-likelihood ({}) found for seed {}'.format(loglik, s))\n",
    "            report_labels(labels,side_info)\n",
    "            best_ls = ls\n",
    "            best_pis = pis\n",
    "            best_labels = labels\n",
    "            best_logliks = logliks\n",
    "    print('Best')\n",
    "    report_labels(best_labels,side_info)\n",
    "    return best_ls, best_pis, best_labels, best_logliks\n",
    "\n",
    "ls_MP, pis_MP, labels_MP, logliks_MP = fit(dataMat_selected, 4, [doc['authors'] for doc in documents])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Another Model\n",
    "\n",
    "In this model we first re-weight our features with a method called **tf-idf**. The main idea of **tf-idf** is that a word which appears in most of the documents, provides less information for classification/clustering; and, therefore, this word should be weighted down.\n",
    "\n",
    "1. **tf(n, f)**: term frequency(counts), in our case, it is the value of the feature(word) f in the document n.\n",
    "\n",
    "\n",
    "2. **df(f)**: document frequency, the number of documents which contain word f.\n",
    "\n",
    "\n",
    "3. **idf(f)**: inverse document-frequency,\n",
    "\n",
    "$$\n",
    "\\text{idf}(f)=\\log \\frac{1+N}{1+df(f)} + 1\n",
    "$$\n",
    "\n",
    "where N is the number of samples (documents).\n",
    "\n",
    "\n",
    "4. **tf-idf** is defined as \n",
    "\n",
    "$$\n",
    "\\text{tf-idf}(n, f) = \\text{tf}(n, f)*\\text{idf}(f)\n",
    "$$\n",
    "\n",
    "5. Each sample after **tf-idf** transformation is normalized with their Euclidean norm\n",
    "\n",
    "$$\n",
    "x_i = \\frac{x_i}{ \\Vert {x_i} \\Vert_2 }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) **[1pt]** Implement tf-idf computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_idf(dataMat):\n",
    "    idfVec = np.zeros((dataMat.shape[1],))\n",
    "    N = dataMat.shape[0]\n",
    "    for i in np.arange(dataMat.shape[1]):\n",
    "        df = np.count_nonzero(dataMat[:,i]>0, axis=0)\n",
    "        idfVec[i] = np.log((1+N)/(1+df))+1\n",
    "    return idfVec\n",
    "test_data = np.array([[1,0,1,0], [0,1,0,0], [1,1,3,0], [2,3,0,4]])\n",
    "res_idf = compute_idf(test_data)\n",
    "assert(np.allclose(res_idf, [1.223143, 1.223143, 1.510825, 1.916290]))\n",
    "\n",
    "def compute_tfidf(dataMat):\n",
    "    data_idf = compute_idf(dataMat)\n",
    "    N = dataMat.shape[0]\n",
    "    dataMat_tfidf = np.zeros(dataMat.shape, dtype='float32')\n",
    "    for i in np.arange(N):\n",
    "        dataMat_tfidf[i,:] = dataMat[i,:]*data_idf\n",
    "        dataMat_tfidf[i,:] = dataMat_tfidf[i,:]/np.sqrt(np.sum(dataMat_tfidf[i,:]**2))\n",
    "    return data_idf, dataMat_tfidf\n",
    "\n",
    "# test function\n",
    "test_data = np.array([[1,0,1,0], [0,1,0,0], [1,1,3,0], [2,3,0,4]], dtype='float32')\n",
    "data_idf, res_tfidf = compute_tfidf(test_data)\n",
    "assert(np.allclose(res_tfidf, [[0.62922752, 0.        , 0.77722114, 0.],\n",
    "                               [0.        , 1.        , 0.        , 0.],\n",
    "                               [0.25212485, 0.25212485, 0.93427306, 0.],\n",
    "                               [0.27662638, 0.41493958, 0.        , 0.86677736]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now transform the data to tf-idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_idf, dataMat_tfidf = compute_tfidf(dataMat)\n",
    "dataMat_tfidf_selected = np.asarray(dataMat_tfidf[:,lst])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Mixture of Gaussian Model\n",
    "\n",
    "Since the data after tf-idf transformed is no longer count data, we should model to Gaussian.\n",
    "\n",
    "1) Multivariate Gaussian pdf\n",
    "\n",
    "$$\n",
    "p(x\\mid\\mu, \\Sigma) = (2 \\pi)^{-\\frac{k}{2}} |\\Sigma|^{-\\frac{1}{2}} \\exp \\{ -\\frac{1}{2} {(x-\\mu)^T \\Sigma^{-1} (x-\\mu)} \\}\n",
    "$$\n",
    "\n",
    "In our model, we set $\\Sigma$ to be a diagonal matrix.\n",
    "\n",
    "2) Notation for the model:\n",
    "  1. $x_i$ be the feature vector of the $i^{th}$ sample, $x_{i, j}$ be the $j^{th}$ features of the $i^{th}$ sample. \n",
    "  2. $h_i$ be the index of the cluster for the $i^{th}$ sample. \n",
    "  3. $\\mu_m$ be the mean vector for the $m^{th}$ cluster, $\\mu_{m,j}$ be the $j^{th}$ feature of the mean vector.\n",
    "  4. $\\Sigma_m$ be the covariance matrix for the $m^{th}$ cluster, $\\sigma^2_{m, j}$ be the $j^{th}$ variance for the $m^{th}$ cluster.\n",
    "  5. $p(h_i = c) = \\pi_c$\n",
    "  \n",
    "3)\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\log p(x_i\\mid \\mu_m, \\Sigma_m ) &= -\\frac{k}{2}\\log(2\\pi)-\\frac{1}{2}\\log(|\\Sigma_m|)-\\frac{1}{2}(x_i-\\mu_m)^T\\Sigma_m^{-1}(x_i-\\mu_m) \\\\\n",
    "&= -\\frac{k}{2}\\log(2\\pi)-\\frac{1}{2}\\log\\Big(\\prod_j \\sigma^2_{m,j}\\Big)-\\frac{1}{2} \\sum_j \\frac{(x_{i,j}-\\mu_{m,j})^2}{\\sigma_{m,j}^2}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions 4-6 are equivalent to the answers you gave in mixture of Poissons. Of course, this model uses different parameters. Hence instead of $\\lambda_m$ you would write $\\mu_m$ and $\\sigma_m^2$\n",
    "\n",
    "4) Log-likelihood of mixture of Gaussian model is:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "LL(\\mu,\\sigma) &= \\sum_i \\log \\sum_c \\pi_c \\left[-\\frac{k}{2}\\log(2\\pi)-\\frac{1}{2}\\log(\\sigma^2_{i})-\\frac{1}{2} \\frac{(x_{i}-\\mu_{i})^2}{\\sigma_{i}^2}\\right] \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Apply Jensen’s inequality to derive lower-bound.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "LL(\\mu, \\sigma^2, \\pi) = \\log\\Big( \\prod_{i} p(x_{i})\\Big) &= \\sum_{i} \\sum_{h_i} q(h_i = m)\\left[ -\\frac{k}{2} \\log (2\\pi)-{\\frac{1}{2}\\log(\\sigma^2_{i})}\\right.\\left. - \\frac{(x_{i}-\\mu_{i})^2}{2\\sigma_{i}^2} \\right]\\\\\n",
    "&- \\sum_{i}\\sum_{h_i}q(h_i = m)\\log q(h_i = m)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) If we let $q(h_i)$ be the posterior probability $p(h_i \\mid x_i, \\mu,\\sigma,\\pi)$, we maximize the lower-bound. Derive posterior formulation. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(h_i = m\\mid x_i, \\mu,\\sigma^2,\\pi) &= \\frac{\\pi_m\\big(-\\frac{k}{2}\\log(2\\pi)-\\frac{1}{2}\\log(|\\Sigma_m|)-\\frac{1}{2}(x_i-\\mu_m)^T\\Sigma_m^{-1}(x_i-\\mu_m)\\big)}{\\sum_c\\pi_c\\big(-\\frac{k}{2}\\log(2\\pi)-\\frac{1}{2}\\log(|\\Sigma_m|)-\\frac{1}{2}(x_i-\\mu_m)^T\\Sigma_m^{-1}(x_i-\\mu_m)\\big)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) **[1pt]**  Write the code to compute posterior. Due to numerical precision, we compute probability in log domain. First implement the function to compute $\\log p(x_i \\mid \\mu_m,\\sigma_m^2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xs: a vector for x_i\n",
    "# mu: a vector for mu\n",
    "# sigma2: a vector for sigma square. Since we assume covariance matrix is diagonal,\n",
    "#         we can use a vector to save the non-zero values\n",
    "# return log probability\n",
    "def logprobvec_MG(xs, mu, sigma2):\n",
    "    lp = (-0.5*mu.shape[0])*np.log(2.0*np.pi)-0.5*np.sum(np.log(sigma2))-0.5*np.sum((xs-mu)**2.0/(sigma2))\n",
    "    return lp\n",
    "\n",
    "# test function\n",
    "test_x = np.array([.2, 2.2])\n",
    "test_mu = np.array([1, 3])\n",
    "test_sigma2 = np.array([2, 0.5])\n",
    "res_v = logprobvec_MG(test_x, test_mu, test_sigma2)\n",
    "assert(np.allclose(res_v, -2.6378770))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xs: an array of shape N*F (N, # samples, F, # features)\n",
    "# mus: an array of shape K*F (K, # clusters, F, # features)\n",
    "# sigma2s: an array of shape K*F (K, # clusters, F, # features)\n",
    "# pis: a vector of shape K\n",
    "# return three variables:\n",
    "# logprobs: an array of shape K*N, the log posterior probabilities in (7), the log probability\n",
    "#           of a sample belong to each cluster\n",
    "# loglik: log-likelihood of the model in (4)\n",
    "# labels: a vector of shape N, the most probable cluster each sample belongs to\n",
    "def logposterior_MG(xs, mus, sigma2s, pis):\n",
    "    K = mus.shape[0]\n",
    "    F = mus.shape[1]\n",
    "    N = xs.shape[0]\n",
    "    assert(xs.shape[1] == F)\n",
    "    logprobs = np.zeros((K,N))\n",
    "    loglik = 0\n",
    "    labels = np.zeros(N)\n",
    "    for n in range(N):\n",
    "        x = xs[n,:]\n",
    "        for k in range(K):\n",
    "            mu = mus[k,:]\n",
    "            sigma2 = sigma2s[k,:]\n",
    "            logprobs[k,n] = np.log(pis[k]) + logprobvec_MG(x,mu,sigma2)\n",
    "        docloglik = logsum(logprobs[:,n]+1e-6)\n",
    "        loglik = loglik + docloglik\n",
    "        logprobs[:,n] = logprobs[:,n] - docloglik\n",
    "        labels[n] = np.argmax(logprobs[:,n])\n",
    "    return logprobs, loglik, labels\n",
    "\n",
    "# test function\n",
    "test_x = np.array([[1.2,.2,.1], [1,2,1], [.5,.6,1]], dtype='float32')\n",
    "test_mu = np.tile(np.round(np.mean(test_x, axis=0)), (3,1))\n",
    "np.random.seed(10)\n",
    "test_mu = test_mu + np.random.randn(test_mu.shape[0], test_mu.shape[1])\n",
    "test_sigma = np.ones(test_mu.shape)\n",
    "test_pis = np.array([0.33, 0.33, 0.33])\n",
    "res_lp, res_ll, res_lab = logposterior_MG(test_x, test_mu, test_sigma, test_pis)\n",
    "assert(np.allclose(res_lp, [[-1.91883423, -2.51793008, -3.58124658],\n",
    "                            [-0.97027882, -0.72769468, -0.98950581],\n",
    "                            [-0.74603191, -0.82930499, -0.51016141]]))\n",
    "assert(np.allclose(res_ll, -11.189608))\n",
    "assert(np.allclose(res_lab, [2,1,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) **[1pt]** Derive the update formulation for $\\mu_{m,j}$. Take the derivative of the lower-bound.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial \\mu_{m, j}} \\sum_{i} \\sum_{c} q(h_i=c) \\log \\left \\{  p(x_{i}, h_i = c) \\right \\} = \\sum_{i} q(h_i = c)(x_{i,j}-\\mu_{m,j})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Let the derivative be zero, we have:\n",
    "\n",
    "$$\n",
    "\\mu_{m, j} = \\frac{\\sum_i q(h_i = c) x_{i,j}}{\\sum_i q(h_i = c)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) **[1pt]** Derive the update formulation for $\\sigma^2_{m,j}$. Take the derivative on the lower-bound derived from (3).\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial \\sigma^2_{m, j}} \\sum_{i} \\sum_{c} q(h_i=c) \\log \\left \\{  p(x_{i}, h_i = c) \\right \\} &=\n",
    "\\sum_i q(h_i = c) \\left( - \\frac{1}{2 \\sigma^2_{m,j}} + \\frac{(x_{i,j} - \\mu_{m,j})^2}{ 2(\\sigma^2_{m,j})^2 }\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Let the derivative be zero, we have:\n",
    "\n",
    "$$\n",
    "\\sigma^2_{m, j} = \\frac{\\sum_i q(h_i = c) (x_{i,j}-\\mu_{m,j})^{2}}{\\sum_i q(h_i = c)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10) Update formulation for $\\pi_m$ is the same as mixture of Poisson model\n",
    "\n",
    "$$\n",
    "\\pi_m = \\frac{\\sum_i q(h_i = m)}{N}\n",
    "$$\n",
    "\n",
    "where N is the number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11) **[1pt]** Write the code to update $\\mu_{m,j}$, $\\sigma^2_{m,j}$ and $\\pi_m$ (hint: the update rule of $\\pi_m$ is the same as mixture of Poisson)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qs: an array of shape K*N, the posterior probabilities in (7), the probability of a sample belong to each cluster\n",
    "# xs: an array of shape N*F (N, # samples, F, # features)\n",
    "# return three variables:\n",
    "# mus: the updated \\mu, an array of shape K*F (K, # clusters, F, # features)\n",
    "# sigma2s: the updated \\sigma^2, an array of shape K*F (K, # clusters, F, # features)\n",
    "# pis: the updated \\pi, a vector of shape K\n",
    "def update_MG(qs, xs):\n",
    "    suff = np.dot(qs,xs)\n",
    "    denom = np.sum(qs,1)[:,np.newaxis]\n",
    "    mus = (suff+1e-6)/(denom+1e-6)\n",
    "    K = qs.shape[0]\n",
    "    sigma2s = np.zeros(mus.shape)\n",
    "    for i in np.arange(K):\n",
    "        suff = np.dot(qs[i,:],(xs - mus[i,:])**2)\n",
    "        sigma2s[i] = (suff+1e-6)/(np.sum(qs[i,:])+1e-6)\n",
    "    sigma2s = sigma2s+1e-6\n",
    "    pis = np.sum(qs,axis=1)\n",
    "    pis = pis/np.sum(qs)\n",
    "    return mus, sigma2s, pis\n",
    "\n",
    "# test function\n",
    "test_qs = np.array([[0.14677797, 0.08062632, 0.02784097],\n",
    "                    [0.37897736, 0.48302123, 0.37176037],\n",
    "                    [0.47424467, 0.43635245, 0.60039866]])\n",
    "test_xs = np.array([[1.2,.2,.1], [1,2,1], [.5,.6,1]], dtype='float32')\n",
    "res_mus, res_sigma2s, res_pis = update_MG(test_qs, test_xs)\n",
    "assert(np.allclose(res_mus, [[1.060471, 0.812210, 0.482459],\n",
    "                             [0.910773, 1.025236, 0.723544],\n",
    "                             [0.864096, 0.878753, 0.717524]]))\n",
    "assert(np.allclose(res_sigma2s, [[0.04661863, 0.66609716, 0.1979422 ],\n",
    "                                 [0.07965803, 0.63567057, 0.17238403],\n",
    "                                 [0.09342444, 0.53853368, 0.17443729]]))\n",
    "assert(np.allclose(res_pis, [0.085081, 0.411252, 0.503665]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting on data of size N: 85 and F: 199\n",
      "\n",
      "run 1\n",
      "\n",
      " A fit with better log-likelihood (63648.389060799775) found for seed 0\n",
      "cluster 0\n",
      "(1 article(s) by DISPUTED), (5 article(s) by HAMILTON), (2 article(s) by JAY), (1 article(s) by MADISON), \n",
      "cluster 1\n",
      "(9 article(s) by DISPUTED), (5 article(s) by HAMILTON), (1 article(s) by JAY), (8 article(s) by MADISON), \n",
      "cluster 2\n",
      "(2 article(s) by DISPUTED), (4 article(s) by HAMILTON), (2 article(s) by JAY), \n",
      "cluster 3\n",
      "(3 article(s) by DISPUTED), (37 article(s) by HAMILTON), (5 article(s) by MADISON), \n",
      "run 2\n",
      "\n",
      " A fit with better log-likelihood (63680.56234214898) found for seed 1\n",
      "cluster 0\n",
      "(1 article(s) by DISPUTED), (30 article(s) by HAMILTON), (1 article(s) by MADISON), \n",
      "cluster 1\n",
      "(5 article(s) by DISPUTED), (3 article(s) by HAMILTON), (3 article(s) by JAY), (4 article(s) by MADISON), \n",
      "cluster 2\n",
      "(9 article(s) by DISPUTED), (12 article(s) by HAMILTON), (9 article(s) by MADISON), \n",
      "cluster 3\n",
      "(6 article(s) by HAMILTON), (2 article(s) by JAY), \n",
      "run 3\n",
      "\n",
      " A fit with better log-likelihood (63856.388878161066) found for seed 2\n",
      "cluster 0\n",
      "(9 article(s) by DISPUTED), (11 article(s) by HAMILTON), (7 article(s) by MADISON), \n",
      "cluster 1\n",
      "(6 article(s) by HAMILTON), (5 article(s) by JAY), \n",
      "cluster 2\n",
      "(5 article(s) by DISPUTED), (1 article(s) by HAMILTON), (4 article(s) by MADISON), \n",
      "cluster 3\n",
      "(1 article(s) by DISPUTED), (33 article(s) by HAMILTON), (3 article(s) by MADISON), \n",
      "run 4\n",
      "run 5\n",
      "run 6\n",
      "run 7\n",
      "run 8\n",
      "run 9\n",
      "run 10\n",
      "run 11\n",
      "\n",
      " A fit with better log-likelihood (63966.7523072105) found for seed 10\n",
      "cluster 0\n",
      "(1 article(s) by DISPUTED), (3 article(s) by HAMILTON), (5 article(s) by JAY), (1 article(s) by MADISON), \n",
      "cluster 1\n",
      "(13 article(s) by DISPUTED), (17 article(s) by HAMILTON), (12 article(s) by MADISON), \n",
      "cluster 2\n",
      "(4 article(s) by HAMILTON), (1 article(s) by MADISON), \n",
      "cluster 3\n",
      "(1 article(s) by DISPUTED), (27 article(s) by HAMILTON), \n",
      "run 12\n",
      "run 13\n",
      "run 14\n",
      "run 15\n",
      "run 16\n",
      "run 17\n",
      "run 18\n",
      "run 19\n",
      "run 20\n",
      "run 21\n",
      "run 22\n",
      "run 23\n",
      "run 24\n",
      "run 25\n",
      "run 26\n",
      "run 27\n",
      "run 28\n",
      "run 29\n",
      "run 30\n",
      "run 31\n",
      "run 32\n",
      "\n",
      " A fit with better log-likelihood (63996.70254912704) found for seed 31\n",
      "cluster 0\n",
      "(4 article(s) by DISPUTED), (11 article(s) by HAMILTON), (7 article(s) by MADISON), \n",
      "cluster 1\n",
      "(11 article(s) by DISPUTED), (5 article(s) by HAMILTON), (7 article(s) by MADISON), \n",
      "cluster 2\n",
      "(30 article(s) by HAMILTON), \n",
      "cluster 3\n",
      "(5 article(s) by HAMILTON), (5 article(s) by JAY), \n",
      "run 33\n",
      "run 34\n",
      "run 35\n",
      "run 36\n",
      "\n",
      " A fit with better log-likelihood (64081.89634324803) found for seed 35\n",
      "cluster 0\n",
      "(3 article(s) by HAMILTON), (5 article(s) by JAY), \n",
      "cluster 1\n",
      "(13 article(s) by DISPUTED), (9 article(s) by HAMILTON), (10 article(s) by MADISON), \n",
      "cluster 2\n",
      "(32 article(s) by HAMILTON), \n",
      "cluster 3\n",
      "(2 article(s) by DISPUTED), (7 article(s) by HAMILTON), (4 article(s) by MADISON), \n",
      "run 37\n",
      "run 38\n",
      "run 39\n",
      "\n",
      " A fit with better log-likelihood (64085.7072716721) found for seed 38\n",
      "cluster 0\n",
      "(13 article(s) by DISPUTED), (3 article(s) by HAMILTON), (12 article(s) by MADISON), \n",
      "cluster 1\n",
      "(1 article(s) by DISPUTED), (4 article(s) by HAMILTON), (5 article(s) by JAY), (1 article(s) by MADISON), \n",
      "cluster 2\n",
      "(1 article(s) by DISPUTED), (37 article(s) by HAMILTON), \n",
      "cluster 3\n",
      "(7 article(s) by HAMILTON), (1 article(s) by MADISON), \n",
      "run 40\n",
      "run 41\n",
      "run 42\n",
      "run 43\n",
      "run 44\n",
      "run 45\n",
      "run 46\n",
      "run 47\n",
      "run 48\n",
      "run 49\n",
      "run 50\n",
      "run 51\n",
      "run 52\n",
      "run 53\n",
      "run 54\n",
      "run 55\n",
      "run 56\n",
      "run 57\n",
      "run 58\n",
      "run 59\n",
      "run 60\n",
      "run 61\n",
      "run 62\n",
      "run 63\n",
      "run 64\n",
      "run 65\n",
      "run 66\n",
      "run 67\n",
      "run 68\n",
      "run 69\n",
      "run 70\n",
      "run 71\n",
      "run 72\n",
      "run 73\n",
      "run 74\n",
      "run 75\n",
      "run 76\n",
      "run 77\n",
      "run 78\n",
      "run 79\n",
      "run 80\n",
      "run 81\n",
      "run 82\n",
      "run 83\n",
      "run 84\n",
      "run 85\n",
      "run 86\n",
      "run 87\n",
      "run 88\n",
      "run 89\n",
      "run 90\n",
      "run 91\n",
      "run 92\n",
      "run 93\n",
      "run 94\n",
      "run 95\n",
      "run 96\n",
      "run 97\n",
      "run 98\n",
      "run 99\n",
      "run 100\n",
      "Best\n",
      "cluster 0\n",
      "(13 article(s) by DISPUTED), (3 article(s) by HAMILTON), (12 article(s) by MADISON), \n",
      "cluster 1\n",
      "(1 article(s) by DISPUTED), (4 article(s) by HAMILTON), (5 article(s) by JAY), (1 article(s) by MADISON), \n",
      "cluster 2\n",
      "(1 article(s) by DISPUTED), (37 article(s) by HAMILTON), \n",
      "cluster 3\n",
      "(7 article(s) by HAMILTON), (1 article(s) by MADISON), \n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "def fit_MG(xs,K,side_info):\n",
    "    F = xs.shape[1]\n",
    "    N = xs.shape[0]\n",
    "    print(\"Fitting on data of size N: {} and F: {}\".format(N, F))\n",
    "    best_loglik = -1e+308\n",
    "    best_logliks = []\n",
    "    best_labels = []\n",
    "    best_mu = []\n",
    "    best_sigma2s = []\n",
    "    best_pis = []\n",
    "    verbose = False\n",
    "    print()\n",
    "    for s in range(100): # run the algorithm 100 times\n",
    "        np.random.seed(s)\n",
    "        mus = np.mean(xs,0) + .01*np.random.randn(K,F) # each time initialize with different lambda\n",
    "        sigma2s = np.ones((K,F))*10\n",
    "        pis = [1./K]*K\n",
    "        logliks = []\n",
    "        for it in range(20): # each time run for 50 iterations\n",
    "            logqs,loglik, labels = logposterior_MG(xs, mus, sigma2s, pis)\n",
    "            qs = np.exp(logqs)\n",
    "            logliks.append(loglik)\n",
    "            mus, sigma2s, pis = update_MG(qs,xs)\n",
    "        if verbose:\n",
    "            print('\\n Seed {}: log-likelihood = {:.5}, best log-likelihood so far = {:.5}'.format(s+1, loglik, best_loglik))\n",
    "        print(\"run \"+str((s+1)))\n",
    "        if loglik > best_loglik:\n",
    "            best_loglik = loglik\n",
    "            print('\\n A fit with better log-likelihood ({}) found for seed {}'.format(loglik, s))\n",
    "            report_labels(labels,side_info)\n",
    "            best_mus = mus\n",
    "            best_sigma2s = sigma2s\n",
    "            best_pis = pis\n",
    "            best_labels = labels\n",
    "            best_logliks = logliks\n",
    "    print('Best')\n",
    "    report_labels(best_labels,side_info)\n",
    "    return best_mus, best_sigma2s, best_pis, best_labels, best_logliks\n",
    "\n",
    "mus_MG, sigma2s_MG, pis_MG, labels_MG, logliks_MG = fit_MG(dataMat_tfidf_selected, 4, [doc['authors'] for doc in documents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Compare Two Clustering Results With Hypergeometric Test\n",
    "\n",
    "Next we will test each cluster for enrichment in articles authored by different people (Hamilton, Madison, Jay). If the p-value is small, then the level of enrichment could not have occurred simply by chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import hypergeom\n",
    "def gen_table(labels, documents):\n",
    "    K = len(np.unique(labels))\n",
    "    authorList = np.asarray([doc['authors'] for doc in documents])\n",
    "    nameList = np.asarray(['JAY', 'MADISON', 'HAMILTON'])\n",
    "    nameList2 = np.asarray(['JAY', 'MADISON', 'HAMILTON', ''])\n",
    "    nameNum = {'JAY': 5, 'MADISON': 14, 'HAMILTON': 51}\n",
    "    enrichment = np.zeros((K,3))\n",
    "    numberTab = np.zeros((K,4))\n",
    "    for i in np.arange(K):\n",
    "        cList = authorList[np.nonzero(labels==i)[0]]\n",
    "        cnt = 0\n",
    "        for j in nameList:\n",
    "            rv = hypergeom(70, nameNum[j], len(cList))\n",
    "            cIntersect = len(np.nonzero(cList==j)[0])\n",
    "            enrichment[i,cnt] = 1-rv.cdf(cIntersect)\n",
    "            numberTab[i,cnt] = cIntersect\n",
    "            cnt = cnt + 1\n",
    "        cnt = 0\n",
    "        for j in nameList2:\n",
    "            cIntersect = len(np.nonzero(cList==j)[0])\n",
    "            numberTab[i, cnt] = cIntersect\n",
    "            cnt = cnt + 1\n",
    "    return enrichment, numberTab\n",
    "\n",
    "def report_table(enrichment, numberTab):\n",
    "    nameList = np.asarray(['JAY', 'MADISON', 'HAMILTON'])\n",
    "    nameList2 = np.asarray(['JAY', 'MADISON', 'HAMILTON', 'DISPUTED'])\n",
    "    print('p-value (smaller indicates more significant enrichment): ')\n",
    "    print('\\t\\t', end=\"\")\n",
    "    for z in nameList:\n",
    "        print('{0:<8}\\t'.format(z), end='')\n",
    "    print()\n",
    "    for i in range(4):\n",
    "        print('cluster {}\\t'.format(i), end='')\n",
    "        for j in range(3):\n",
    "            print('{0:3f}\\t'.format(enrichment[i,j]), end='')\n",
    "        print()\n",
    "\n",
    "    print('\\n\\ncount table: ')\n",
    "    print('\\t\\t', end=\"\")\n",
    "    for z in nameList2:\n",
    "        print('{0:<8}\\t'.format(z), end='')\n",
    "    print()\n",
    "    for i in range(4):\n",
    "        print('cluster {}\\t'.format(i), end='')\n",
    "        for j in range(4):\n",
    "            print('{0:3f}\\t'.format(numberTab[i,j]), end='')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering Results of Mixture of Poissons:\n",
      "p-value (smaller indicates more significant enrichment): \n",
      "\t\tJAY     \tMADISON \tHAMILTON\t\n",
      "cluster 0\t0.000961\t0.993311\t0.919420\t\n",
      "cluster 1\t0.654044\t0.077191\t0.754442\t\n",
      "cluster 2\t0.945633\t0.998012\t0.004683\t\n",
      "cluster 3\t0.534303\t0.001177\t1.000000\t\n",
      "\n",
      "\n",
      "count table: \n",
      "\t\tJAY     \tMADISON \tHAMILTON\tDISPUTED\t\n",
      "cluster 0\t4.000000\t0.000000\t11.000000\t4.000000\t\n",
      "cluster 1\t0.000000\t4.000000\t8.000000\t1.000000\t\n",
      "cluster 2\t0.000000\t1.000000\t26.000000\t3.000000\t\n",
      "cluster 3\t1.000000\t9.000000\t6.000000\t7.000000\t\n"
     ]
    }
   ],
   "source": [
    "print('Clustering Results of Mixture of Poissons:')\n",
    "enrichment, numberTab = gen_table(labels_MP, documents)\n",
    "report_table(enrichment, numberTab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering Results of Mixture of Gaussian:\n",
      "p-value (smaller indicates more significant enrichment): \n",
      "\t\tJAY     \tMADISON \tHAMILTON\t\n",
      "cluster 0\t0.929714\t0.000008\t1.000000\t\n",
      "cluster 1\t0.000000\t0.700810\t0.993397\t\n",
      "cluster 2\t0.983361\t0.999998\t0.000000\t\n",
      "cluster 3\t0.465340\t0.505597\t0.067451\t\n",
      "\n",
      "\n",
      "count table: \n",
      "\t\tJAY     \tMADISON \tHAMILTON\tDISPUTED\t\n",
      "cluster 0\t0.000000\t12.000000\t3.000000\t13.000000\t\n",
      "cluster 1\t5.000000\t1.000000\t4.000000\t1.000000\t\n",
      "cluster 2\t0.000000\t0.000000\t37.000000\t1.000000\t\n",
      "cluster 3\t0.000000\t1.000000\t7.000000\t0.000000\t\n"
     ]
    }
   ],
   "source": [
    "print('Clustering Results of Mixture of Gaussian:')\n",
    "enrichment, numberTab = gen_table(labels_MG, documents)\n",
    "report_table(enrichment, numberTab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12) **[1pt]** State your conclusion about who is most likely to have authored disputed articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the p-values from both models, it seems **Madison** is most likely to have authored disputed articles. That is to <br> say, Madison has the smallest p-value $(0.001177)$ for the mixture of Poissons cluster $($number $3)$ where most of the <br> disputed values are found, and also the smallest p-value $(0.000008)$ for the mixture of Gaussian cluster $($number $0)$, <br> where most of the disputed values are found."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
